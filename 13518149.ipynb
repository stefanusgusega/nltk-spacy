{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c69643d",
   "metadata": {},
   "source": [
    "# PR NLP NLTK - Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41acc43c",
   "metadata": {},
   "source": [
    "Eksplorasi *library* yang biasa digunakan untuk Natural Language Processing, yaitu **NLTK** dan **Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12a77f",
   "metadata": {},
   "source": [
    "## Author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b37c59",
   "metadata": {},
   "source": [
    "Stefanus Gusega Gunawan - 13518149"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4a348",
   "metadata": {},
   "source": [
    "## Import packages dan inisiasi package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "919a0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Load small pretreained model yang disediakan oleh Spacy untuk bahasa Inggris\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb58f6",
   "metadata": {},
   "source": [
    "## Sentence Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e869e",
   "metadata": {},
   "source": [
    "Mengecek apakah sebuah penanda (determiner: “.”, “?”, “!”) adalah akhir sebuah kalimat (EOS) atau tidak. Hal ini diimplementasikan oleh `sent_tokenize` pada NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c2407ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh kumpulan kalimat yang akan ditokenize menjadi kalimat-kalimat\n",
    "sentences = 'The same words in a different order can mean something completely different. God is great! What can you do? I won a lottery!          '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3719222",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ebf8d4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The same words in a different order can mean something completely different.',\n",
       " 'God is great!',\n",
       " 'What can you do?',\n",
       " 'I won a lottery!']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dengan menggunakan function pada NLTK bernama sent_tokenize akan menghasilkan array of sentence\n",
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d372d0b",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0ea30952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The same words in a different order can mean something completely different.\n",
      "God is great!\n",
      "What can you do?\n",
      "I won a lottery!          \n"
     ]
    }
   ],
   "source": [
    "# Aplikasikan pretrained model Spacy pada kumpulan kalimat\n",
    "doc = nlp(sentences)\n",
    "# Cetak tiap kalimat dan pastikan kalau EOS terdeteksi dengan benar\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f60d41",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b6591",
   "metadata": {},
   "source": [
    "Membagi kalimat menjadi token-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dec8a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Tokenization is the U.K. process of segmenting a string of characters into words.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0b9c7",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ee8b1fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'U.K.',\n",
       " 'process',\n",
       " 'of',\n",
       " 'segmenting',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'characters',\n",
       " 'into',\n",
       " 'words',\n",
       " '.']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menggunakan fungsi word_tokenize dari NLTK untuk mensegmentasi kalimat menjadi kata-kata beserta punctuationnya\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c3bbd",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ae289aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'U.K.',\n",
       " 'process',\n",
       " 'of',\n",
       " 'segmenting',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'characters',\n",
       " 'into',\n",
       " 'words',\n",
       " '.']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplikasikan pretreained model\n",
    "doc = nlp(sentence)\n",
    "# Print hasil tokenization\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9096c7d",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae6909",
   "metadata": {},
   "source": [
    "Stemming di sini maksudnya adalah menghapus suffix, prefix, dan/atau infix dari suatu kata, sehingga meninggalkan *stemmed word*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c7f3d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = [\n",
    "    'program',\n",
    "    'programs',\n",
    "    'programming',\n",
    "    'programmer',\n",
    "    'programmers',\n",
    "    'write',\n",
    "    'writes',\n",
    "    'written',\n",
    "    'wrote'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602fed6",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d1509462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "program\n",
      "program\n",
      "programm\n",
      "programm\n",
      "write\n",
      "write\n",
      "written\n",
      "wrote\n"
     ]
    }
   ],
   "source": [
    "# Inisiasi algoritma Porter untuk proses stemming yang menggunakan suffix stripping\n",
    "nltk_stemmer = PorterStemmer()\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print(nltk_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d9c19025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "program\n",
      "program\n",
      "programm\n",
      "programm\n",
      "write\n",
      "write\n",
      "written\n",
      "wrote\n"
     ]
    }
   ],
   "source": [
    "# Inisiasi algoritma stemming lainnya yang bernama Snowball Stemmer\n",
    "nltk_stemmer = SnowballStemmer('english')\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print(nltk_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebcb308",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd640be",
   "metadata": {},
   "source": [
    "Sayangnya Spacy tidak menyediakan fitur untuk melakukan *stemming*, maka dari itu lebih baik gunakan NLTK untuk melakukan *stemming*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7e602",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03c29a",
   "metadata": {},
   "source": [
    "Jika melihat *stemming* menghasilkan kata dasar tapi tidak terdefinisi dari kamus, di sinilah *lemmatization* dibutuhkan. Kata dasar yang dikembalikan saat proses *lemmatization* adalah kata dasar yang terdefinisi di kamus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "93b7dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_to_lemmatize = [\n",
    "    'program',\n",
    "    'programs',\n",
    "    'programming',\n",
    "    'write',\n",
    "    'writes',\n",
    "    'written',\n",
    "    'wrote',\n",
    "]\n",
    "\n",
    "nouns_to_lemmatize = [\n",
    "    'rocks',\n",
    "    'corpus',\n",
    "    'ashes',\n",
    "    'knives'\n",
    "]\n",
    "\n",
    "adjectives_to_lemmatize = [\n",
    "    'good',\n",
    "    'better',\n",
    "    'best',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa013a",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d38b4516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "program\n",
      "program\n",
      "write\n",
      "write\n",
      "write\n",
      "write\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Melakukan lemmatization sebagai verb\n",
    "for word in verbs_to_lemmatize:\n",
    "    print(nltk_lemmatizer.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6fbaa345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock\n",
      "corpus\n",
      "ash\n",
      "knife\n"
     ]
    }
   ],
   "source": [
    "# Melakukan lemmatization sebagai noun by default\n",
    "for word in nouns_to_lemmatize:\n",
    "    print(nltk_lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e9d71f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "# Melakukan lemmatization sebagai adjective by default\n",
    "for word in adjectives_to_lemmatize:\n",
    "    print(nltk_lemmatizer.lemmatize(word, 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf02c18",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d3800a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-', 'be', 'read', 'the', 'newspaper']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lakukan parsing kalimat dengan menggunakan pretrained model\n",
    "doc = nlp('I was reading the newspaper')\n",
    "# Print masing-masing lemmatisasi dari tiap hasil parse kalimat\n",
    "[word.lemma_ for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "52e65556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock']\n",
      "['corpus']\n",
      "['ashe']\n",
      "['knife']\n"
     ]
    }
   ],
   "source": [
    "# Iterasi tiap kata benda pada array\n",
    "for noun in nouns_to_lemmatize:\n",
    "    # Lakukan parsing dengan pretrained model\n",
    "    doc = nlp(noun)\n",
    "    # Print hasil lemmatisasi\n",
    "    print([word.lemma_ for word in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3f04ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program']\n",
      "['program']\n",
      "['programming']\n",
      "['write']\n",
      "['write']\n",
      "['write']\n",
      "['write']\n"
     ]
    }
   ],
   "source": [
    "for verb in verbs_to_lemmatize:\n",
    "    doc = nlp(verb)\n",
    "    print([word.lemma_ for word in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "92eab1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good']\n",
      "['well']\n",
      "['good']\n"
     ]
    }
   ],
   "source": [
    "for adj in adjectives_to_lemmatize:\n",
    "    doc = nlp(adj)\n",
    "    print([word.lemma_ for word in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943aaa9b",
   "metadata": {},
   "source": [
    "## Entity Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b844c",
   "metadata": {},
   "source": [
    "Suatu teknik untuk seakan-akan melakukan sensor terhadap entitas-entitas tertentu, seperti email, nama orang, nama brand, dan sebagainya. Di sini hanya digunakan Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ae220c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text=\"\"\"Indian man has allegedly duped nearly 50 businessmen in the UAE of USD 1.6 million and fled the country in the most unlikely way -- on a repatriation flight to Hyderabad, according to a media report on Saturday.Yogesh Ashok Yariava, the prime accused in the fraud, flew from Abu Dhabi to Hyderabad on a Vande Bharat repatriation flight on May 11 with around 170 evacuees, the Gulf News reported.Yariava, the 36-year-old owner of the fraudulent Royal Luck Foodstuff Trading, made bulk purchases worth 6 million dirhams (USD 1.6 million) against post-dated cheques from unsuspecting traders before fleeing to India, the daily said.\n",
    "The bought goods included facemasks, hand sanitisers, medical gloves (worth nearly 5,00,000 dirhams), rice and nuts (3,93,000 dirhams), tuna, pistachios and saffron (3,00,725 dirhams), French fries and mozzarella cheese (2,29,000 dirhams), frozen Indian beef (2,07,000 dirhams) and halwa and tahina (52,812 dirhams).\n",
    "The list of items and defrauded persons keeps getting longer as more and more victims come forward, the report said.\n",
    "The aggrieved traders have filed a case with the Bur Dubai police station.\n",
    "The traders said when the dud cheques started bouncing they rushed to the Royal Luck's office in Dubai but the shutters were down, even the fraudulent company's warehouses were empty.\"\"\"\n",
    "\n",
    "sentence = 'To email Guido, try guido@python.org or the older address guido@google.com'\n",
    "\n",
    "news_doc = nlp(news_text)\n",
    "sent_doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "28a4c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk melakukan masking terhadap named entity menjadi tipe entitasnya, dan menambahkan kasus khusus jika itu adalah email,\n",
    "# maka masking dengan menggunakan __EMAIL__\n",
    "def remove_details(word):\n",
    "    # Jika tipe entitas nya PERSON, ORG, dan GPE, maka kembalikan entitas tipe untuk masking\n",
    "    if word.ent_type_ =='PERSON' or word.ent_type_=='ORG' or word.ent_type_=='GPE':\n",
    "        return f' _{word.ent_type_}_ '\n",
    "    # Jika memiliki struktur email, maka kembalikan _EMAIL_\n",
    "    elif word.like_email:\n",
    "        return ' _EMAIL_ '\n",
    "    # Jika bukan termasuk dua di atas, maka kembalikan bentuk awalnya\n",
    "    return word.string\n",
    "\n",
    "def update_text(doc):\n",
    "    # Lakukan mapping terhadap parsed text\n",
    "    tokens = map(remove_details, doc)\n",
    "    # dan gabungkan\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "19a206dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Indian man has allegedly duped nearly 50 businessmen in the UAE of USD 1.6 million and fled the country in the most unlikely way -- on a repatriation flight to  _GPE_ , according to a media report on Saturday. _PERSON_  _PERSON_  _PERSON_ , the prime accused in the fraud, flew from  _GPE_  _GPE_ to  _GPE_ on a Vande Bharat repatriation flight on May 11 with around 170 evacuees,  _ORG_  _ORG_  _ORG_ reported. _ORG_ , the 36-year-old owner of the fraudulent Royal Luck Foodstuff Trading, made bulk purchases worth 6 million dirhams (USD 1.6 million) against post-dated cheques from unsuspecting traders before fleeing to  _GPE_ , the daily said.\\nThe bought goods included facemasks, hand sanitisers, medical gloves (worth nearly 5,00,000 dirhams), rice and nuts (3,93,000 dirhams), tuna, pistachios and saffron (3,00,725 dirhams), French fries and mozzarella cheese (2,29,000 dirhams), frozen Indian beef (2,07,000 dirhams) and halwa and tahina (52,812 dirhams).\\nThe list of items and defrauded persons keeps getting longer as more and more victims come forward, the report said.\\nThe aggrieved traders have filed a case with the  _ORG_  _ORG_ police station.\\nThe traders said when the dud cheques started bouncing they rushed to  _ORG_  _ORG_  _ORG_  _ORG_ office in  _GPE_ but the shutters were down, even the fraudulent company's warehouses were empty.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entity masking text berita\n",
    "update_text(news_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6f21453e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To email  _PERSON_ , try  _EMAIL_ or the older address  _EMAIL_ '"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entity masking teks yang mengandung email\n",
    "update_text(sent_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4bb3a5",
   "metadata": {},
   "source": [
    "## POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb22da",
   "metadata": {},
   "source": [
    "Sebuah kalimat pasti memiliki struktur kalimat atau *syntax* tersendiri. POS Tagger atau Part-Of-Speech Tagger berguna untuk melabeli tiap-tiap bagian dari sebuah kalimat termasuk syntax apa. Contoh dari POS seperti *noun*, *plural noun*, *verb*, dan sebagainya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d8deb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Can you please buy me a Luwak White Coffee over there? It's only Rp15,000.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bebef1",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "589f40b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Can', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('please', 'VB'),\n",
       " ('buy', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('Luwak', 'NNP'),\n",
       " ('White', 'NNP'),\n",
       " ('Coffee', 'NNP'),\n",
       " ('over', 'IN'),\n",
       " ('there', 'RB'),\n",
       " ('?', '.'),\n",
       " ('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('only', 'RB'),\n",
       " ('Rp15,000', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tagger baru bisa digunakan setelah kalimat sudah ditokenisasi\n",
    "nltk.pos_tag(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9d238",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "806ba7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can, POS: VERB, POS Tag: MD\n",
      "you, POS: PRON, POS Tag: PRP\n",
      "please, POS: INTJ, POS Tag: UH\n",
      "buy, POS: VERB, POS Tag: VB\n",
      "me, POS: PRON, POS Tag: PRP\n",
      "a, POS: DET, POS Tag: DT\n",
      "Luwak, POS: PROPN, POS Tag: NNP\n",
      "White, POS: PROPN, POS Tag: NNP\n",
      "Coffee, POS: PROPN, POS Tag: NNP\n",
      "over, POS: ADV, POS Tag: RB\n",
      "there, POS: ADV, POS Tag: RB\n",
      "?, POS: PUNCT, POS Tag: .\n",
      "It, POS: PRON, POS Tag: PRP\n",
      "'s, POS: AUX, POS Tag: VBZ\n",
      "only, POS: ADV, POS Tag: RB\n",
      "Rp15,000, POS: PROPN, POS Tag: NNP\n",
      "., POS: PUNCT, POS Tag: .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "# Print text hasil tokenisasi, part of speech secara umum, dan POS Tag nya yang lebih menjelaskan peran POS\n",
    "for word in doc:\n",
    "    print(f\"{word.text}, POS: {word.pos_}, POS Tag: {word.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96366a6",
   "metadata": {},
   "source": [
    "## Phrase Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce5a28",
   "metadata": {},
   "source": [
    "Untuk mengekstraksi frase yang mengikuti pola tertentu. Pola biasanya dilambangkan dengan menggunakan RegEx yang berisikan POS Tag. Untuk contoh kalimat di bawah, akan diambil informasi kata benda dan juga kata sifat yang menggambarkan kata benda itu jika ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1a5beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Can you please buy me a cup of cincau over there? It's only Rp15,000 and it's a delicious coffee that I've ever drink.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed1370",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cacb10a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [('Can', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('buy', 'VB'), ('me', 'PRP'), Tree('Chunk', [('a', 'DT'), ('cup', 'NN')]), ('of', 'IN'), Tree('Chunk', [('cincau', 'NN')]), ('over', 'IN'), ('there', 'RB'), ('?', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('only', 'RB'), ('Rp15,000', 'NNP'), ('and', 'CC'), ('it', 'PRP'), (\"'s\", 'VBZ'), Tree('Chunk', [('a', 'DT'), ('delicious', 'JJ'), ('coffee', 'NN')]), ('that', 'IN'), ('I', 'PRP'), (\"'ve\", 'VBP'), ('ever', 'RB'), ('drink', 'VBP'), ('.', '.')])\n"
     ]
    }
   ],
   "source": [
    "# Definisikan pattern yang akan dicari\n",
    "# 0 atau 1 determiner, 0 atau tak terhingga adjective, dan 1 kata benda\n",
    "pattern = \"Chunk: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# Inisiasi regex parser dengan pattern tadi\n",
    "parser = nltk.RegexpParser(pattern)\n",
    "\n",
    "# Lalu, parse kalimat yang sudah ditokenisasi\n",
    "result = parser.parse(nltk.pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# untuk menampilkan representasi pohon pada terminal\n",
    "print(result.__repr__())\n",
    "\n",
    "# Untuk menampilkan gambar pohon\n",
    "result.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
